{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadab689-3497-4195-b867-95c036484b2a",
   "metadata": {},
   "source": [
    "# Submitting a Ray Data Pipeline as a RayJob to an Existing Cluster\n",
    "\n",
    "In this notebook, we will demonstrate how to use the CodeFlare SDK to submit a Ray Data pipeline as a **RayJob** to an existing Ray cluster. Specifically, we will:\n",
    "\n",
    "- Authenticate to our OpenShift/Kubernetes environment\n",
    "- Define a RayJob that runs a document processing pipeline\n",
    "- Submit the job to an **existing** Ray cluster\n",
    "- Monitor the job status and retrieve logs\n",
    "\n",
    "This approach is ideal for production workflows where you have a long-running Ray cluster and want to submit batch jobs without managing cluster lifecycle.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "The submitted job (`ray_data_process.py`) uses [Docling](https://github.com/DS4SD/docling) and Ray Data to:\n",
    "- Read PDF files from a Persistent Volume Claim (PVC)\n",
    "- Convert PDFs to JSON format using distributed actor pools\n",
    "- Save results to the same PVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23c378-b38b-49b4-998a-5d4c908e0a0a",
   "metadata": {},
   "source": [
    "## Import SDK Components\n",
    "\n",
    "We import the core components from the CodeFlare SDK:\n",
    "- `RayJob`: Submits and manages Ray jobs on existing clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d185d13-f2f4-4f14-bea9-d467a9849c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk import RayJob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac86e1c",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "Configure authentication to your OpenShift cluster. If you're running within an OpenShift environment with default kubeconfig, authentication may be automatic. Otherwise, provide your token and server URL.\n",
    "\n",
    "> **Note**: Replace the token and server values with your own credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705caf-01aa-40fa-ae01-f905cf9f51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the below `oc login` command using your Token and Server URL. Ensure the command is prepended by `!` and not `%`. This will work when running both locally and within RHOAI.\n",
    "!oc login --token=\"<TOKEN>\" --server=\"<SERVER_URL>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0b16b",
   "metadata": {},
   "source": [
    "## Create the Pipeline Script\n",
    "\n",
    "First, we'll create the `ray_data_process.py` script that will be submitted as the RayJob. This script is **optimized for maximum throughput**:\n",
    "\n",
    "| Optimization | Implementation | Impact |\n",
    "|--------------|----------------|--------|\n",
    "| **One-time Model Loading** | `DoclingProcessor` loads Docling models once per actor | Avoids ~10s startup per file |\n",
    "| **Parallel PVC Writes** | Each actor writes directly to PVC | N actors = N concurrent writes |\n",
    "| **Streaming Execution** | Read ‚Üí Process ‚Üí Write stages overlap via `iter_batches()` | Keeps all actors busy |\n",
    "| **Prefetching** | `prefetch_batches=2` keeps data ready for actors | Eliminates I/O waits |\n",
    "| **Configurable Scaling** | `MIN_ACTORS`/`MAX_ACTORS` env vars | Tune for your cluster size |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80545c-195a-44ca-82ca-d77626f80bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ray_data_process_async.py\n",
    "\"\"\"\n",
    "Ray Data Pipeline: PDF to JSON Conversion\n",
    "\n",
    "This script uses an DoclingProcessor to overlap CPU-heavy document conversion\n",
    "with I/O-heavy PVC (Persistent Volume Claim) writes. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ray\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# --- PVC Configuration ---\n",
    "PVC_MOUNT_PATH = os.environ.get(\"PVC_MOUNT_PATH\", \"/mnt/data\")\n",
    "INPUT_PATH = os.environ.get(\"INPUT_PATH\", \"input/pdfs\")\n",
    "OUTPUT_PATH = os.environ.get(\"OUTPUT_PATH\", \"output\")\n",
    "\n",
    "# --- Performance Tuning Parameters ---\n",
    "NUM_FILES = int(os.environ.get(\"NUM_FILES\", \"1000\"))\n",
    "MIN_ACTORS = int(os.environ.get(\"MIN_ACTORS\", \"8\"))\n",
    "MAX_ACTORS = int(os.environ.get(\"MAX_ACTORS\", \"8\"))\n",
    "CPUS_PER_ACTOR = int(os.environ.get(\"CPUS_PER_ACTOR\", \"8\"))\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"1\"))\n",
    "\n",
    "class DoclingProcessor:\n",
    "    \"\"\"\n",
    "    Stateful Ray Actor that handles PDF conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        import socket\n",
    "        from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "        from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorOptions\n",
    "        from docling.datamodel.base_models import InputFormat\n",
    "        from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "\n",
    "        # Hardware-level thread tuning\n",
    "        os.environ[\"OMP_NUM_THREADS\"] = str(CPUS_PER_ACTOR)\n",
    "        os.environ[\"MKL_NUM_THREADS\"] = str(CPUS_PER_ACTOR)\n",
    "        \n",
    "        self.hostname = socket.gethostname()\n",
    "        self.processed_count = 0\n",
    "\n",
    "        # Initialize Docling Converter\n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.do_ocr = False\n",
    "        pipeline_options.do_table_structure = True\n",
    "        pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "            num_threads=CPUS_PER_ACTOR,\n",
    "            device=\"cpu\"\n",
    "        )\n",
    "        \n",
    "        pdf_format_config = PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "            backend=PyPdfiumDocumentBackend\n",
    "        )\n",
    "\n",
    "        self.converter = DocumentConverter(\n",
    "            format_options={InputFormat.PDF: pdf_format_config}\n",
    "        )\n",
    "        \n",
    "        # Prepare output paths\n",
    "        self.output_base = Path(PVC_MOUNT_PATH) / OUTPUT_PATH\n",
    "        self.json_dir = self.output_base / \"json\"\n",
    "        self.json_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"[{self.hostname}] üü¢ Actor initialized (CPUs: {CPUS_PER_ACTOR})\")\n",
    "\n",
    "    def __call__(self, batch: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        Processes a batch of PDFs. \n",
    "        \"\"\"\n",
    "        from docling.datamodel.base_models import DocumentStream\n",
    "        import io\n",
    "        import orjson\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for file_bytes, file_path in zip(batch[\"bytes\"], batch[\"path\"]):\n",
    "            fname = os.path.basename(file_path)\n",
    "            fname_base = fname.rsplit('.', 1)[0]\n",
    "            status = \"success\"\n",
    "            error_msg = \"\"\n",
    "            page_count = 0\n",
    "            docling_duration = 0.0\n",
    "            output_size_bytes = 0\n",
    "\n",
    "            try:\n",
    "                if not file_bytes or len(file_bytes) < 100:\n",
    "                    raise ValueError(\"File empty or too small\")\n",
    "\n",
    "                docling_start = time.time()\n",
    "                \n",
    "                # --- Step 1: Heavy CPU Conversion ---\n",
    "                stream = DocumentStream(name=fname, stream=io.BytesIO(file_bytes))\n",
    "                doc = self.converter.convert(stream)\n",
    "\n",
    "                page_count = len(doc.document.pages) if hasattr(doc.document, 'pages') else 0\n",
    "                #md_out = doc.document.export_to_markdown()\n",
    "                json_out = doc.document.export_to_dict()\n",
    "                \n",
    "                docling_duration = time.time() - docling_start\n",
    "\n",
    "                # --- Step 2: Parallel I/O Writes ---\n",
    "                json_path = self.json_dir / f\"{fname_base}.json\"\n",
    "                json_bytes = orjson.dumps(json_out, option=orjson.OPT_INDENT_2)\n",
    "\n",
    "                output_size_bytes = len(json_bytes)\n",
    "                self._write_with_retry(json_path, json_bytes)\n",
    "\n",
    "                \n",
    "                self.processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                status = \"error\"\n",
    "                error_msg = str(e)[:150]\n",
    "\n",
    "            results.append({\n",
    "                \"filename\": str(fname),\n",
    "                \"status\": str(status),\n",
    "                \"page_count\": int(page_count),  \n",
    "                \"error\": str(error_msg),\n",
    "                \n",
    "                # Timing\n",
    "                \"docling_duration_s\": float(round(docling_duration, 2)),\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \n",
    "                # Size metrics\n",
    "                \"file_size_mb\": float(round(len(file_bytes) / (1024 * 1024), 3)),\n",
    "                \"output_size_kb\": float(round(len(orjson.dumps(json_out)) / 1024, 2)) if status == \"success\" else 0.0,\n",
    "                \n",
    "                # Efficiency\n",
    "                \"pages_per_second\": float(round(page_count / docling_duration, 2)) if docling_duration > 0 else 0.0,\n",
    "                \n",
    "                # Distribution tracking\n",
    "                \"actor_hostname\": self.hostname,\n",
    "            })\n",
    "\n",
    "        return {\"results\": results}\n",
    "    \n",
    "    def _write_with_retry(self, path: Path, content: str, max_retries: int = 3):\n",
    "        \"\"\"Worker-local disk write with retry logic.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                path.write_bytes(content)\n",
    "                return\n",
    "            except Exception:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(1)\n",
    "\n",
    "def ray_data_process():\n",
    "    input_full_path = os.path.join(PVC_MOUNT_PATH, INPUT_PATH)\n",
    "    # Stage 1: Set Global Parallelism\n",
    "    # SET GLOBAL PARALLELISM BEFORE READING\n",
    "    ctx = ray.data.DataContext.get_current()\n",
    "    ctx.execution_options.preserve_order = False\n",
    "    ctx.execution_options.actor_locality_enabled = True\n",
    "    ctx.min_parallelism = 100 \n",
    "    ctx.target_min_block_size = 1 * 1024 * 1024\n",
    "    ctx.target_max_block_size = 2 * 1024 * 1024  # 2 MB\n",
    "    ctx.target_shuffle_block_size = 1 * 1024 * 1024\n",
    "    target_blocks = MAX_ACTORS * 4\n",
    "\n",
    "    # READ WITH EXPLICIT PARALLELISM\n",
    "    ds = ray.data.read_binary_files(\n",
    "        input_full_path, \n",
    "        include_paths=True,\n",
    "        override_num_blocks=target_blocks\n",
    "    )\n",
    "    \n",
    "    ds = ds.filter(lambda row: row[\"path\"].lower().endswith(\".pdf\")).limit(NUM_FILES)\n",
    "    ds = ds.repartition(num_blocks=target_blocks, shuffle=False)\n",
    "\n",
    "    # Stage 2: Map with Actor Pool\n",
    "    results_ds = ds.map_batches(\n",
    "        DoclingProcessor,\n",
    "        compute=ray.data.ActorPoolStrategy(\n",
    "            min_size=MIN_ACTORS,\n",
    "            max_size=MAX_ACTORS,\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_cpus=CPUS_PER_ACTOR,\n",
    "    )\n",
    "    print(f\"DEBUG: Target blocks: {target_blocks}, Max actors: {MAX_ACTORS}\")    \n",
    "    \n",
    "    # Stage 3: Collect & Report\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    total_pages = 0\n",
    "    total_docling_time = 0.0\n",
    "    start_time = time.time()\n",
    "    # Additional tracking variables\n",
    "    total_file_size_mb = 0.0\n",
    "    total_output_size_kb = 0.0\n",
    "    actor_distribution = {}\n",
    "    file_durations = []\n",
    "    errors_list = []\n",
    "    \n",
    "    for batch in results_ds.iter_batches(batch_size=10, prefetch_batches=2):\n",
    "        for result_list in batch.get(\"results\", []):\n",
    "            items = result_list if isinstance(result_list, list) else [result_list]\n",
    "            for item in items:\n",
    "                if item[\"status\"] == \"success\":\n",
    "                    success_count += 1\n",
    "                    file_durations.append((item[\"filename\"], item[\"docling_duration_s\"]))\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    errors_list.append((item[\"filename\"], item.get(\"error\", \"\")))\n",
    "                \n",
    "                total_pages += item[\"page_count\"]\n",
    "                total_docling_time += item[\"docling_duration_s\"]\n",
    "                total_file_size_mb += item.get(\"file_size_mb\", 0)\n",
    "                total_output_size_kb += item.get(\"output_size_kb\", 0)\n",
    "                \n",
    "                # Track actor distribution\n",
    "                actor = item.get(\"actor_hostname\", \"unknown\")\n",
    "                actor_distribution[actor] = actor_distribution.get(actor, 0) + 1\n",
    "\n",
    "    # Calculate derived metrics\n",
    "    wall_clock = time.time() - start_time\n",
    "    total_files = success_count + error_count\n",
    "    error_rate = (error_count / total_files * 100) if total_files > 0 else 0\n",
    "    avg_pages_per_file = total_pages / success_count if success_count > 0 else 0\n",
    "    parallelization_efficiency = (total_docling_time / wall_clock / MAX_ACTORS * 100) if wall_clock > 0 else 0\n",
    "    \n",
    "    # Sort for fastest/slowest\n",
    "    if file_durations:\n",
    "        file_durations.sort(key=lambda x: x[1])\n",
    "        fastest = file_durations[0]\n",
    "        slowest = file_durations[-1]\n",
    "    \n",
    "    # Enhanced Report\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PERFORMANCE REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n--- Results Summary ---\")\n",
    "    print(f\"Total Files:    {total_files}\")\n",
    "    print(f\"Success:        {success_count} ({100-error_rate:.1f}%)\")\n",
    "    print(f\"Errors:         {error_count} ({error_rate:.1f}%)\")\n",
    "    print(f\"Total Pages:    {total_pages}\")\n",
    "    print(f\"Avg Pages/File: {avg_pages_per_file:.1f}\")\n",
    "    \n",
    "    print(\"\\n--- Data Volume ---\")\n",
    "    print(f\"Input Size:     {total_file_size_mb:.2f} MB\")\n",
    "    print(f\"Output Size:    {total_output_size_kb/1024:.2f} MB\")\n",
    "    print(f\"Compression:    {total_output_size_kb*1024/total_file_size_mb/1024/1024:.1f}x\" if total_file_size_mb > 0 else \"N/A\")\n",
    "    \n",
    "    print(\"\\n--- Timing ---\")\n",
    "    print(f\"Wall Clock:     {wall_clock:.2f}s\")\n",
    "    print(f\"CPU Time (sum): {total_docling_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\n--- Throughput ---\")\n",
    "    print(f\"Files/second:   {success_count / wall_clock:.2f}\")\n",
    "    print(f\"Pages/second:   {total_pages / wall_clock:.2f}\")\n",
    "    print(f\"MB/second:      {total_file_size_mb / wall_clock:.2f}\")\n",
    "    \n",
    "    print(\"\\n--- Efficiency ---\")\n",
    "    print(f\"Parallelization: {parallelization_efficiency:.1f}% of ideal\")\n",
    "    print(f\"Speedup:         {total_docling_time / wall_clock:.1f}x vs sequential\")\n",
    "    \n",
    "    if file_durations:\n",
    "        print(\"\\n--- Outliers ---\")\n",
    "        print(f\"Fastest: {fastest[0]} ({fastest[1]:.2f}s)\")\n",
    "        print(f\"Slowest: {slowest[0]} ({slowest[1]:.2f}s)\")\n",
    "    \n",
    "    print(\"\\n--- Actor Distribution ---\")\n",
    "    for actor, count in sorted(actor_distribution.items()):\n",
    "        pct = count / total_files * 100\n",
    "        print(f\"  {actor}: {count} files ({pct:.1f}%)\")\n",
    "    \n",
    "    if errors_list:\n",
    "        print(\"\\n--- Errors (first 5) ---\")\n",
    "        for fname, err in errors_list[:5]:\n",
    "            print(f\"  {fname}: {err[:60]}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    ray_data_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169cebb",
   "metadata": {},
   "source": [
    "## OpenShift/Kubernetes PVC Requirements\n",
    "\n",
    "To use PVC storage with your RayCluster, you need to:\n",
    "\n",
    "1. **Create a PVC** with `ReadWriteMany` (RWX) access mode\n",
    "2. **Mount the PVC** on both the Ray head and worker pods\n",
    "3. **Upload your PDF files** to the PVC before running the job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "beea19d8-0b1e-4165-b5ff-d90d4b29a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and namespace configuration\n",
    "EXISTING_CLUSTER_NAME = \"raytest\"  # Name of your existing Ray cluster\n",
    "NAMESPACE = \"ray-docling\"        # Namespace where the cluster is running\n",
    "\n",
    "# ============================================\n",
    "# PVC Configuration\n",
    "# ============================================\n",
    "PVC_NAME = \"my-rwx-pvc2\"         # Name of your PVC (must already exist)\n",
    "PVC_MOUNT_PATH = \"/mnt/data\"      # Mount path inside Ray pods\n",
    "INPUT_PATH = \"input/pdfs\"         # Subdirectory for input PDFs\n",
    "OUTPUT_PATH = \"output\"            # Subdirectory for output files\n",
    "\n",
    "# ============================================\n",
    "# Verify PVC exists\n",
    "# ============================================\n",
    "import subprocess\n",
    "\n",
    "print(f\"üîç Checking PVC '{PVC_NAME}' in namespace '{NAMESPACE}'...\")\n",
    "result = subprocess.run(\n",
    "    [\"oc\", \"get\", \"pvc\", PVC_NAME, \"-n\", NAMESPACE, \"-o\", \"jsonpath={.status.phase}\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"‚ùå PVC '{PVC_NAME}' not found in namespace '{NAMESPACE}'\")\n",
    "    print(f\"   Error: {result.stderr}\")\n",
    "    print(f\"\\n   Create the PVC first using the YAML in the configuration section below.\")\n",
    "else:\n",
    "    pvc_status = result.stdout\n",
    "    print(f\"‚úÖ PVC '{PVC_NAME}' found, status: {pvc_status}\")\n",
    "    \n",
    "    # Get access mode\n",
    "    result = subprocess.run(\n",
    "        [\"oc\", \"get\", \"pvc\", PVC_NAME, \"-n\", NAMESPACE, \"-o\", \"jsonpath={.spec.accessModes[0]}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    access_mode = result.stdout\n",
    "    print(f\"   Access Mode: {access_mode}\")\n",
    "    \n",
    "    if access_mode != \"ReadWriteMany\":\n",
    "        print(f\"   ‚ö†Ô∏è  Warning: PVC should use 'ReadWriteMany' access mode for concurrent writes from multiple workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e71d02",
   "metadata": {},
   "source": [
    "## Configure the RayJob\n",
    "\n",
    "Create the RayJob with **performance tuning parameters**:\n",
    "\n",
    "| Parameter | Description | Tuning Guidance |\n",
    "|-----------|-------------|-----------------|\n",
    "| `MAX_ACTORS` | Maximum parallel actors | Set to `total_cluster_cpus / CPUS_PER_ACTOR` |\n",
    "| `MIN_ACTORS` | Warm actors (avoids cold start) | 2-4 for steady workloads |\n",
    "| `CPUS_PER_ACTOR` | CPUs per Docling actor | 2 for most PDFs, 4 for complex documents |\n",
    "| `BATCH_SIZE` | PDFs per actor batch | 1 for large PDFs, 2-4 for small PDFs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b14d7-8df7-4406-a06d-fda3e1aa283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚ö° PERFORMANCE TUNING PARAMETERS\n",
    "# ============================================\n",
    "# Adjust these based on your cluster size for maximum throughput\n",
    "\n",
    "# How many files to process\n",
    "NUM_FILES = \"1000\"\n",
    "\n",
    "# Actor pool sizing (CRITICAL for throughput!)\n",
    "# Formula: MAX_ACTORS ‚âà (total_worker_cpus) / CPUS_PER_ACTOR\n",
    "# Example: 4 workers √ó 8 CPUs each = 32 CPUs ‚Üí MAX_ACTORS = 16 with 2 CPUs each\n",
    "MIN_ACTORS = \"8\"    # Keep minimum actors warm (avoids cold start)\n",
    "MAX_ACTORS = \"8\"    # Scale up based on cluster (increase for larger clusters!)\n",
    "\n",
    "# CPUs per Docling actor\n",
    "CPUS_PER_ACTOR = \"8\"\n",
    "\n",
    "# PDFs per batch (1 for large PDFs, 2-4 for small PDFs < 1MB)\n",
    "BATCH_SIZE = \"1\"\n",
    "\n",
    "# ============================================\n",
    "# Create the RayJob Configuration\n",
    "# ============================================\n",
    "# Note: The PVC must already be mounted on the RayCluster pods\n",
    "\n",
    "rayjob = RayJob(\n",
    "    job_name=\"ray-data-process-pvc\",\n",
    "    cluster_name=EXISTING_CLUSTER_NAME,\n",
    "    namespace=NAMESPACE,\n",
    "    entrypoint=\"python ray_data_process_async.py\",\n",
    "    runtime_env={\n",
    "        \"working_dir\": \".\",\n",
    "        \"pip\": [\"opencv-python-headless\",\"pypdfium2\", \"orjson\"],\n",
    "        \"env_vars\": {\n",
    "            # PVC configuration\n",
    "            \"PVC_MOUNT_PATH\": PVC_MOUNT_PATH,\n",
    "            \"INPUT_PATH\": INPUT_PATH,\n",
    "            \"OUTPUT_PATH\": OUTPUT_PATH,\n",
    "            # Performance tuning\n",
    "            \"NUM_FILES\": NUM_FILES,\n",
    "            \"MIN_ACTORS\": MIN_ACTORS,\n",
    "            \"MAX_ACTORS\": MAX_ACTORS,\n",
    "            \"CPUS_PER_ACTOR\": CPUS_PER_ACTOR,\n",
    "            \"BATCH_SIZE\": BATCH_SIZE,\n",
    "            # üí° Enable detailed progress monitoring\n",
    "            \"RAY_DATA_ENABLE_RICH_PROGRESS_BARS\": \"true\",\n",
    "            \"RAY_record_task_actor_creation_sites\": \"true\",\n",
    "            \"RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION\": \"0.5\",\n",
    "            # Cache directories\n",
    "            \"HF_HOME\": \"/tmp/huggingface\",\n",
    "            \"XDG_CACHE_HOME\": \"/tmp/cache\"\n",
    "        }\n",
    "    },\n",
    "    active_deadline_seconds=7200,     # Timeout after 2 hours\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ RayJob configured: {rayjob.name}\")\n",
    "print(f\"   ‚Üí Cluster: {EXISTING_CLUSTER_NAME}\")\n",
    "print(f\"   ‚Üí PVC Mount: {PVC_MOUNT_PATH}\")\n",
    "print(f\"   ‚Üí Input Path: {PVC_MOUNT_PATH}/{INPUT_PATH}\")\n",
    "print(f\"   ‚Üí Output Path: {PVC_MOUNT_PATH}/{OUTPUT_PATH}\")\n",
    "print(f\"   ‚Üí Actors:  {MIN_ACTORS}-{MAX_ACTORS} √ó {CPUS_PER_ACTOR} CPUs each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf9209",
   "metadata": {},
   "source": [
    "## Submit the RayJob\n",
    "\n",
    "Submit the job to the existing Ray cluster. This creates a RayJob custom resource in Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e35972bf-2097-45ef-934d-f9c06f69c82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rayjob.submit()\n",
    "print(f\"üöÄ Job submitted! Timeout set to {rayjob.active_deadline_seconds}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ec8d",
   "metadata": {},
   "source": [
    "## Monitor Job Status\n",
    "\n",
    "Check the status of the submitted RayJob. Re-run this cell to see updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8be8ccde-d773-4c34-8154-188db6da61c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rayjob.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7c99af",
   "metadata": {},
   "source": [
    "## Retrieve Job Logs\n",
    "\n",
    "View the job logs via `oc` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logs from the RayJob submitter pod\n",
    "!oc logs -l job-name={rayjob.name} -n {NAMESPACE} --tail=200\n",
    "\n",
    "# Alternative: get logs from the Ray head pod\n",
    "# !oc logs -l ray.io/cluster={EXISTING_CLUSTER_NAME} -c ray-head -n {NAMESPACE} --tail=200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
