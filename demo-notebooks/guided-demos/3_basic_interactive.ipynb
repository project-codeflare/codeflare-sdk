{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc21043",
   "metadata": {},
   "source": [
    "In this fourth and final notebook, we will go over how to leverage the SDK to directly work interactively with a Ray cluster during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bc3ea-4ce3-49bf-bb1f-e209de8ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.cluster.auth import TokenAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614daa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authentication object for oc user permissions\n",
    "auth = TokenAuthentication(\n",
    "    token = \"XXXXX\",\n",
    "    server = \"XXXXX\",\n",
    "    skip_tls=False\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27f84c",
   "metadata": {},
   "source": [
    "Once again, let's start by running through the same cluster setup as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bc870-091f-4e11-9642-cba145710159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name='interactivetest',\n",
    "    namespace='default',\n",
    "    min_worker=2,\n",
    "    max_worker=2,\n",
    "    min_cpus=2,\n",
    "    max_cpus=2,\n",
    "    min_memory=8,\n",
    "    max_memory=8,\n",
    "    gpu=1,\n",
    "    image=\"quay.io/project-codeflare/ray:2.5.0-py38-cu116\",\n",
    "    instascale=True,\n",
    "    machine_types=[\"m5.xlarge\", \"g4dn.xlarge\"]\n",
    "    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0884bbc-c224-4ca0-98a0-02dfa09c2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.up()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33663f47",
   "metadata": {},
   "source": [
    "This time we will demonstrate another potential method of use: working with the Ray cluster interactively.\n",
    "\n",
    "Using the SDK, we can get both the Ray cluster URI and dashboard URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1719bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_dashboard_uri = cluster.cluster_dashboard_uri()\n",
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "print(ray_dashboard_uri)\n",
    "print(ray_cluster_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aca6a",
   "metadata": {},
   "source": [
    "Now we can connect directly to our Ray cluster via the Ray python client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before proceeding make sure the cluster exists and the uri is not empty\n",
    "assert ray_cluster_uri, \"Ray cluster needs to be started and set before proceeding\"\n",
    "\n",
    "import ray\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "# reset the ray context in case there's already one. \n",
    "ray.shutdown()\n",
    "# establish connection to ray cluster\n",
    "\n",
    "#install additional libraries that will be required for model training\n",
    "runtime_env = {\"pip\": [\"transformers\", \"datasets\", \"evaluate\", \"pyarrow<7.0.0\", \"accelerate\"]}\n",
    "\n",
    "ray.init(address=f'{ray_cluster_uri}', runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711030b",
   "metadata": {},
   "source": [
    "Now that we are connected (and have passed in some package requirements), let's try writing some training code for a DistilBERT transformer model via HuggingFace (using IMDB dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_fn():\n",
    "    from datasets import load_dataset\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, TrainingArguments\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    import numpy as np\n",
    "    from datasets import load_metric\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.train.huggingface import HuggingFaceTrainer\n",
    "\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    #using a fraction of dataset but you can run with the full dataset\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "    print(f\"len of train {small_train_dataset} and test {small_eval_dataset}\")\n",
    "\n",
    "    ray_train_ds = ray.data.from_huggingface(small_train_dataset)\n",
    "    ray_evaluation_ds = ray.data.from_huggingface(small_eval_dataset)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    def trainer_init_per_worker(train_dataset, eval_dataset, **config):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "        training_args = TrainingArguments(\"/tmp/hf_imdb/test\", eval_steps=1, disable_tqdm=True, \n",
    "                                          num_train_epochs=1, skip_memory_metrics=True,\n",
    "                                          learning_rate=2e-5,\n",
    "                                          per_device_train_batch_size=16,\n",
    "                                          per_device_eval_batch_size=16,                                \n",
    "                                          weight_decay=0.01,)\n",
    "        return transformers.Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "    scaling_config = ScalingConfig(num_workers=2, use_gpu=True) #num workers is the number of gpus\n",
    "\n",
    "    # we are using the ray native HuggingFaceTrainer, but you can swap out to use non ray Huggingface Trainer. Both have the same method signature. \n",
    "    # the ray native HFTrainer has built in support for scaling to multiple GPUs\n",
    "    trainer = HuggingFaceTrainer(\n",
    "        trainer_init_per_worker=trainer_init_per_worker,\n",
    "        scaling_config=scaling_config,\n",
    "        datasets={\"train\": ray_train_ds, \"evaluation\": ray_evaluation_ds},\n",
    "    )\n",
    "    result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8fd65",
   "metadata": {},
   "source": [
    "Once we want to test our code out, we can run the training function we defined above remotely on our Ray cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the above cell as a remote ray function\n",
    "ray.get(train_fn.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8cd32",
   "metadata": {},
   "source": [
    "Once complete, we can bring our Ray cluster down and clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36db0f-31f6-4373-9503-dc3c1c4c3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.logout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
