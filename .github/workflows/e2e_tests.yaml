# e2e tests workflow for CodeFlare-SDK
name: e2e

on:
  pull_request:
    branches:
      - main
      - 'release-*'
    paths-ignore:
      - 'docs/**'
      - '**.adoc'
      - '**.md'
      - 'LICENSE'

concurrency:
  group: ${{ github.head_ref }}-${{ github.workflow }}
  cancel-in-progress: true

env:
  CODEFLARE_OPERATOR_IMG: "quay.io/project-codeflare/codeflare-operator:dev"

jobs:
  kubernetes:
    runs-on: gpu-t4-4-core

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Checkout common repo code
        uses: actions/checkout@v4
        with:
          repository: 'project-codeflare/codeflare-common'
          ref: 'main'
          path: 'common'

      - name: Checkout CodeFlare operator repository
        uses: actions/checkout@v4
        with:
          repository: project-codeflare/codeflare-operator
          path: codeflare-operator

      - name: Set Go
        uses: actions/setup-go@v5
        with:
          go-version-file: './codeflare-operator/go.mod'
          cache-dependency-path: "./codeflare-operator/go.sum"

      - name: Set up gotestfmt
        uses: gotesttools/gotestfmt-action@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up specific Python version
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # caching pip dependencies

      - name: Setup NVidia GPU environment for KinD
        uses: ./common/github-actions/nvidia-gpu-setup

      - name: Setup and start KinD cluster
        uses: ./common/github-actions/kind
        with:
          worker-nodes: 1

      - name: Install NVidia GPU operator for KinD
        uses: ./common/github-actions/nvidia-gpu-operator

      - name: Verify GPU availability in KinD
        run: |
          echo "Checking for available GPUs in the KinD cluster..."
          
          # Wait for GPU operator pods to be ready (with timeout)
          echo "Waiting for GPU operator pods to be ready..."
          TIMEOUT=300  # 5 minutes timeout
          END=$((SECONDS + TIMEOUT))
          
          while [ $SECONDS -lt $END ]; do
            # Get total number of pods in the namespace
            TOTAL_PODS=$(kubectl get pods -n gpu-operator --no-headers | wc -l)
            
            # Count pods that are either running and ready or completed successfully
            # Exclude pods that are still initializing
            READY_PODS=$(kubectl get pods -n gpu-operator --no-headers | grep -E 'Running|Completed' | grep -v 'PodInitializing' | wc -l)
            
            if [ "$READY_PODS" -eq "$TOTAL_PODS" ] && [ "$TOTAL_PODS" -gt 0 ]; then
              echo "All GPU operator pods are ready or completed successfully!"
              break
            fi
            
            echo "Waiting for GPU operator pods to be ready... ($READY_PODS/$TOTAL_PODS)"
            echo "Pod status:"
            kubectl get pods -n gpu-operator
            sleep 10
          done
          
          if [ $SECONDS -ge $END ]; then
            echo "::error::Timeout waiting for GPU operator pods to be ready"
            echo "GPU operator pod status:"
            kubectl get pods -n gpu-operator -o wide
            echo "GPU operator pod logs:"
            kubectl logs -n gpu-operator -l app.kubernetes.io/name=gpu-operator
            echo "GPU operator pod events:"
            kubectl get events -n gpu-operator
            exit 1
          fi
          
          echo "Node details:"
          kubectl describe nodes | grep -E 'nvidia.com/gpu|Allocatable:|Capacity:|Name:'
          
          # Check if GPU operator has labeled nodes
          GPU_LABELS=$(kubectl describe nodes | grep -c "nvidia.com/gpu")
          if [ "$GPU_LABELS" -eq 0 ]; then
            echo "::error::No NVIDIA GPU labels found on nodes. GPU operator may not be running correctly."
            echo "Full node descriptions for debugging:"
            kubectl describe nodes
            exit 1
          fi
          
          # Check if GPUs are actually allocatable
          GPU_ALLOCATABLE=$(kubectl get nodes -o jsonpath='{.items[*].status.allocatable.nvidia\.com/gpu}' | tr ' ' '\n' | grep -v '^$' | wc -l)
          if [ "$GPU_ALLOCATABLE" -eq 0 ]; then
            echo "::error::GPU operator is running but no GPUs are allocatable. Check GPU operator logs."
            echo "Checking GPU operator pods:"
            kubectl get pods -n gpu-operator -o wide
            echo "GPU operator pod logs:"
            kubectl logs -n gpu-operator -l app.kubernetes.io/name=gpu-operator
            echo "GPU operator pod events:"
            kubectl get events -n gpu-operator
            echo "GPU operator pod descriptions:"
            kubectl describe pods -n gpu-operator
            exit 1
          fi
          
          echo "Successfully found $GPU_ALLOCATABLE allocatable GPU(s) in the cluster."

      - name: Deploy CodeFlare stack
        id: deploy
        run: |
          cd codeflare-operator
          echo Setting up CodeFlare stack
          make setup-e2e
          
          # Create ConfigMap to disable mTLS
          echo "Creating ConfigMap to disable mTLS..."
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: codeflare-operator-config
            namespace: ray-system
          data:
            config.yaml: |
              kuberay:
                mTLSEnabled: false
                rayDashboardOAuthEnabled: false  
                ingressDomain: "kind"
              appwrapper:
                enabled: true
          EOF
          
          echo Deploying CodeFlare operator
          make deploy -e IMG="${CODEFLARE_OPERATOR_IMG}" -e ENV="e2e"
          kubectl wait --timeout=120s --for=condition=Available=true deployment -n openshift-operators codeflare-operator-manager
          cd ..

      - name: Add user to KinD
        uses: ./common/github-actions/kind-add-user
        with:
          user-name: sdk-user

      - name: Grant sdk-user port-forwarding permissions
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: port-forward-permissions
          rules:
          - apiGroups: [""]
            resources: ["services", "pods"]
            verbs: ["get", "list", "watch"]
          - apiGroups: [""]
            resources: ["pods/portforward"]
            verbs: ["create"]
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: sdk-user-port-forward-binding
          subjects:
          - kind: User
            name: sdk-user
            apiGroup: rbac.authorization.k8s.io
          roleRef:
            kind: ClusterRole
            name: port-forward-permissions
            apiGroup: rbac.authorization.k8s.io
          EOF
        shell: bash

      - name: Configure RBAC for sdk user with limited permissions
        run: |
          kubectl create clusterrole list-ingresses --verb=get,list --resource=ingresses
          kubectl create clusterrolebinding sdk-user-list-ingresses --clusterrole=list-ingresses --user=sdk-user
          kubectl create clusterrole namespace-creator --verb=get,list,create,delete,patch --resource=namespaces
          kubectl create clusterrolebinding sdk-user-namespace-creator --clusterrole=namespace-creator --user=sdk-user
          kubectl create clusterrole raycluster-creator --verb=get,list,create,delete,patch --resource=rayclusters
          kubectl create clusterrolebinding sdk-user-raycluster-creator --clusterrole=raycluster-creator --user=sdk-user
          kubectl create clusterrole appwrapper-creator --verb=get,list,create,delete,patch --resource=appwrappers
          kubectl create clusterrolebinding sdk-user-appwrapper-creator --clusterrole=appwrapper-creator --user=sdk-user
          kubectl create clusterrole resourceflavor-creator --verb=get,list,create,delete --resource=resourceflavors
          kubectl create clusterrolebinding sdk-user-resourceflavor-creator --clusterrole=resourceflavor-creator --user=sdk-user
          kubectl create clusterrole clusterqueue-creator --verb=get,list,create,delete,patch --resource=clusterqueues
          kubectl create clusterrolebinding sdk-user-clusterqueue-creator --clusterrole=clusterqueue-creator --user=sdk-user
          kubectl create clusterrole localqueue-creator --verb=get,list,create,delete,patch --resource=localqueues
          kubectl create clusterrolebinding sdk-user-localqueue-creator --clusterrole=localqueue-creator --user=sdk-user
          kubectl create clusterrole list-secrets --verb=get,list --resource=secrets
          kubectl create clusterrolebinding sdk-user-list-secrets --clusterrole=list-secrets --user=sdk-user
          kubectl create clusterrole pod-creator --verb=get,list --resource=pods
          kubectl create clusterrolebinding sdk-user-pod-creator --clusterrole=pod-creator --user=sdk-user
          kubectl config use-context sdk-user

      - name: Run e2e tests
        run: |
          export CODEFLARE_TEST_OUTPUT_DIR=${{ env.TEMP_DIR }}
          echo "CODEFLARE_TEST_OUTPUT_DIR=${CODEFLARE_TEST_OUTPUT_DIR}" >> $GITHUB_ENV

          set -euo pipefail
          pip install poetry
          poetry install --with test,docs
          echo "Running e2e tests..."
          poetry run pytest -v -s ./tests/e2e -m 'kind and nvidia_gpu' > ${CODEFLARE_TEST_OUTPUT_DIR}/pytest_output.log 2>&1
        env:
          GRPC_DNS_RESOLVER: "native"

      - name: Switch to kind-cluster context to print logs
        if: always() && steps.deploy.outcome == 'success'
        run: kubectl config use-context kind-cluster

      - name: Print Pytest output log
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Printing Pytest output logs"
          cat ${CODEFLARE_TEST_OUTPUT_DIR}/pytest_output.log

      - name: Print CodeFlare operator logs
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Printing CodeFlare operator logs"
          kubectl logs -n openshift-operators --tail -1 -l app.kubernetes.io/name=codeflare-operator | tee ${CODEFLARE_TEST_OUTPUT_DIR}/codeflare-operator.log

      - name: Print KubeRay operator logs
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Printing KubeRay operator logs"
          kubectl logs -n ray-system --tail -1 -l app.kubernetes.io/name=kuberay | tee ${CODEFLARE_TEST_OUTPUT_DIR}/kuberay.log

      - name: Export all KinD pod logs
        uses: ./common/github-actions/kind-export-logs
        if: always() && steps.deploy.outcome == 'success'
        with:
          output-directory: ${CODEFLARE_TEST_OUTPUT_DIR}

      - name: Upload logs
        uses: actions/upload-artifact@v4
        if: always() && steps.deploy.outcome == 'success'
        with:
          name: logs
          retention-days: 10
          path: |
            ${{ env.CODEFLARE_TEST_OUTPUT_DIR }}/**/*.log
